{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "pacific_raw = requests.get(\"https://www.nhc.noaa.gov/data/hurdat/hurdat2-nepac-1949-2018-122019.txt\")\n",
    "pacific_raw.raise_for_status()  # check that we actually got something back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'EP': 1050, '19': 18050, 'CP': 77, '20': 10520})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for line in io.StringIO(pacific_raw.text).readlines():\n",
    "    c[line[:2]] += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms_r = []\n",
    "pacific_storm_r = {'header': None, 'data': []}\n",
    "\n",
    "for i, line in enumerate(io.StringIO(pacific_raw.text).readlines()):\n",
    "    if line[:2] == 'EP' or line[:2] == 'CP':\n",
    "        pacific_storms_r.append(pacific_storm_r.copy())\n",
    "        pacific_storm_r['header'] = line\n",
    "        pacific_storm_r['data'] = []\n",
    "    else:\n",
    "        pacific_storm_r['data'].append(line)\n",
    "\n",
    "pacific_storms_r = pacific_storms_r[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pacific_storm_dfs = []\n",
    "for storm_dict in pacific_storms_r:\n",
    "    storm_id, storm_name, storm_entries_n = storm_dict['header'].split(\",\")[:3]\n",
    "    # remove hanging newline ('\\n'), split fields\n",
    "    data = [[entry.strip() for entry in datum[:-1].split(\",\")] for datum in storm_dict['data']]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame['id'] = storm_id\n",
    "    frame['name'] = storm_name\n",
    "    pacific_storm_dfs.append(frame)\n",
    "\n",
    "pacific_storms = pd.concat(pacific_storm_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms = pacific_storms.reindex(columns=pacific_storms.columns[-2:] | pacific_storms.columns[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms.columns = [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"date\",\n",
    "        \"hours_minutes\",\n",
    "        \"record_identifier\",\n",
    "        \"status_of_system\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"maximum_sustained_wind_knots\",\n",
    "        \"maximum_pressure\",\n",
    "        \"34_kt_ne\",\n",
    "        \"34_kt_se\",\n",
    "        \"34_kt_sw\",\n",
    "        \"34_kt_nw\",\n",
    "        \"50_kt_ne\",\n",
    "        \"50_kt_se\",\n",
    "        \"50_kt_sw\",\n",
    "        \"50_kt_nw\",\n",
    "        \"64_kt_ne\",\n",
    "        \"64_kt_se\",\n",
    "        \"64_kt_sw\",\n",
    "        \"64_kt_nw\",\n",
    "        \"na\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pacific_storms['na']\n",
    "\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pacific_storms = pacific_storms.replace(to_replace='-999', value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    28419\n",
       "L        105\n",
       "I          6\n",
       "T          4\n",
       "S          3\n",
       "Name: record_identifier, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pacific_storms = pacific_storms.replace(to_replace=\"\", value=np.nan)\n",
    "pacific_storms['record_identifier'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pacific_storms['latitude'] = pacific_storms['latitude'].map(lambda lat: lat[:-1] if lat[-1] == \"N\" else -lat[:-1])\n",
    "pacific_storms['longitude']= pacific_storms['longitude'].map(lambda long: long[:-1] if long[-1] == \"E\" else \"-\" + long[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms['date'] = pd.to_datetime(pacific_storms['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms['date'] = pacific_storms\\\n",
    "    .apply(\n",
    "        lambda srs: srs['date'].replace(hour=int(srs['hours_minutes'][:2]), minute=int(srs['hours_minutes'][2:])), \n",
    "        axis='columns'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pacific_storms['hours_minutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms['name'] = pacific_storms['name'].map(lambda n: n.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms.index = range(len(pacific_storms.index))\n",
    "pacific_storms.index.name = \"index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_storms.to_csv(\"../pacific_storms.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "atlantic_raw = requests.get(\"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2018-120319.txt\")\n",
    "atlantic_raw.raise_for_status()  # check that we actually got something back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for line in io.StringIO(atlantic_raw.text).readlines():\n",
    "    c[line[:2]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "atlantic_storms_r = []\n",
    "atlantic_storm_r = {'header': None, 'data': []}\n",
    "\n",
    "for i, line in enumerate(io.StringIO(atlantic_raw.text).readlines()):\n",
    "    if line[:2] == 'AL':\n",
    "        atlantic_storms_r.append(atlantic_storm_r.copy())\n",
    "        atlantic_storm_r['header'] = line\n",
    "        atlantic_storm_r['data'] = []\n",
    "    else:\n",
    "        atlantic_storm_r['data'].append(line)\n",
    "\n",
    "atlantic_storms_r = atlantic_storms_r[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "atlantic_storm_dfs = []\n",
    "for storm_dict in atlantic_storms_r:\n",
    "    storm_id, storm_name, storm_entries_n = storm_dict['header'].split(\",\")[:3]\n",
    "    # remove hanging newline ('\\n'), split fields\n",
    "    data = [[entry.strip() for entry in datum[:-1].split(\",\")] for datum in storm_dict['data']]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame['id'] = storm_id\n",
    "    frame['name'] = storm_name\n",
    "    atlantic_storm_dfs.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms = pd.concat(atlantic_storm_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms = atlantic_storms.reindex(columns=atlantic_storms.columns[-2:] | atlantic_storms.columns[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms.columns = [\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"date\",\n",
    "        \"hours_minutes\",\n",
    "        \"record_identifier\",\n",
    "        \"status_of_system\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"maximum_sustained_wind_knots\",\n",
    "        \"maximum_pressure\",\n",
    "        \"34_kt_ne\",\n",
    "        \"34_kt_se\",\n",
    "        \"34_kt_sw\",\n",
    "        \"34_kt_nw\",\n",
    "        \"50_kt_ne\",\n",
    "        \"50_kt_se\",\n",
    "        \"50_kt_sw\",\n",
    "        \"50_kt_nw\",\n",
    "        \"64_kt_ne\",\n",
    "        \"64_kt_se\",\n",
    "        \"64_kt_sw\",\n",
    "        \"64_kt_nw\",\n",
    "        \"na\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del atlantic_storms['na']\n",
    "pd.set_option(\"max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "atlantic_storms = atlantic_storms.replace(to_replace='-999', value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms = atlantic_storms.replace(to_replace=\"\", value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms['latitude'] = atlantic_storms['latitude'].map(lambda lat: lat[:-1] if lat[-1] == \"N\" else -lat[:-1])\n",
    "atlantic_storms['longitude']= atlantic_storms['longitude'].map(lambda long: long[:-1] if long[-1] == \"E\" else \"-\" + long[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms['date'] = pd.to_datetime(atlantic_storms['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms['date'] = atlantic_storms\\\n",
    "    .apply(\n",
    "        lambda srs: srs['date'].replace(hour=int(srs['hours_minutes'][:2]), minute=int(srs['hours_minutes'][2:])), \n",
    "        axis='columns'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del atlantic_storms['hours_minutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms['name'] = atlantic_storms['name'].map(lambda n: n.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_storms.index = range(len(atlantic_storms.index))\n",
    "atlantic_storms.index.name = \"index\"\n",
    "atlantic_storms.to_csv(\"../atlantic_storms.csv\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
